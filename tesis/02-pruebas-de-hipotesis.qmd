# Pruebas de hipótesis {.unnumbered}

Uno de los pasos del **Análisis Exploratorio de Datos Espaciales** (_ESDA_ por
sus siglas en inglés) es probar si los datos geográficos cuentan con algún
tipo de estructura espacial. Hacerlo es importante ya que si en efecto existe
estructura espacial en los datos, entonces esta se debe incorporar en todo
el análisis.

Sin embargo, antes de entrar en detalle sobre qué es la estructura espacial
primero es importante definir su contraparte, la **aleatorieda espacial**
(_spatial randomness_).

## Aleatoriedad espacial

Es la ausencia de cualquier tipo de patrón espacial en los datos, y debe
satisfacer dos condiciones:

* Las observaciones son equiprobables de ocurrir en cualquer ubicación en el
espacio.
* El valor de una observación no depende de sus vecinos.

Para propósitos de análisis, es de interés encontrar evidencia en contra de
aleatoriedad espacial ya que esto indicaría que existe estructura espacial
en lo datos.

## Base de pruebas de hipótesis

Dado que es de interés encontrar evidencia en contra de aleatoriedad espacial,
entonces es posible apalancarse de la [1° ley geográfica de Tobler](https://en.wikipedia.org/wiki/Tobler%27s_first_law_of_geography)
para el planteamiento de una prueba hipótesis, misma que establece que en un
espacio geográfico "_Todo está relacionado con todo, pero las cosas más cercanas aún más_".
Bajo esta ley se pueden extraer dos conclusiones importantes:

1. En un contexto geográfico hay estructuras con dependencia espacial.
2. Existe decaemiento en la correlación de las cosas con la distancia, es
decir, observaciones más separadas estarán menos asociadas.

Estos puntos resultan en el planteamiento de la **prueba de hipótesis** básica
para datos geoespaciales:

* $H_0$: Los datos se distribuyen bajo aleatoriedad espacial.
* $H_1$: Los datos presentan algún tipo de estructura espacial.

Misma que se fundamenta en determinar la **autocorrelación espacial** de los
datos.

## Autocorrelación espacial

Busca medir la variación de una misma variable en ubicaciones distintas, y
puede tomar valores tanto positivos como negativos. Esto lleva a la
interpretación de tres posibles casos:

![Casos de autocorrelación espacial](./figuras/01/autocorrelacion_espacial.png)

* Autocorrelación **positiva**: Se da cuando las observaciones de una vecindad
tienen valores similares sean negativos (_cold spot_) o positivos (_hot spot_),
con mayor frecuencia que en aleatoriedad espacial, lo cual resulta en la
presencia de **segmentos geográficos** (_clusters_).

* Autocorrelación **nula**: No existe autocorrelación en los datos por lo que
hay aleatoriedad espacial.

* Autocorrelación **negativa**: Se da cuando las observaciones de una vecindad
tienen **valores alternantes** con mayor frecuencia que en aleatoriedad
espacial, lo cual resulta en estructuras tipo tablero de ajedrez.

### Estadísticos para autocorrelación espacial

Recordando que un **estadístico** es cualquier valor que resume alguna
característica de una distribución de referencia ($H_0$), entonces un
**estadístico de prueba** es aquel que se calcula de los datos y se compara con
esa misma distribución buscando responder a la pregunta:

>¿Qué tan probable es obtener el estadístico de prueba si éste proviniera
>de datos generados de la distribución de referencia $H_0$?

Éste entonces acepta o rechaza $H_0$ en función de si la probabilidad de
observar esos datos supera o no la región de rechazo en la distribución de
referencia.

Para construir un estadístico de prueba en un contexto espacial es
necesario que éste capture tanto la **similitud de atributos** como la
**similitud de ubicación** de una variable consigo misma pero en ubicaciones
distintas, de ahí que sea **auto**correlación en contraste con otros
estadísticos como pudiera ser la correlación de Pearson.

#### Similitud de atributos

Busca resumir la similitud de diferentes observaciones de una misma variable
$x$ en diferentes ubicaciones $i$, $j$, de modo que es una función
$f(x_i, x_j)$

#### Similitud de ubicación

Formaliza la noción de un vecino a través de los **pesos espaciales** 
($w_{ij}$) que se almacenan en la matriz de pesos espaciales $W$ a través
de algún tipo de interacción.

Con estas dos partes se puede construir un estadístico $z$ que sume a lo
largo de todos los pares de observaciones $(i, j)$ el producto de la similitud
de atributos con la similitud de ubicación, de modo que:

$$z = \sum_{ij}f(x_i,x_j)\cdot w_{ij}$$

::: {.callout-important}
A la hora de plantear estos enfoques se deriva una complicación fundamental
debida al **problema de parámetros incidentales**. Para entenderlo, se debe
recordar que la autocorrelación espacial se sustenta en definir la interacción
entre pares de observaciones para todas las $n\cdot(\frac{n-1}{2})$ duplas
posibles en un espacio geográfico.

La relación entre todos los posibles pares de observaciones se almacena en una
matriz de interacciones de dimensiones $n\times n$ siendo cada elemento un
parámetro del problema. Esto hace evidente que para un conjunto de $n$
observaciones, el número de parámetros crecería con un factor de $n^2$.
:::

### Pesos espaciales

La solución al problema anterior radica en imponer algún tipo de estructura a
los datos para limitar el número de parámetros por estimar, es aquí en donde
los **pesos espaciales** desempeñan un papel esencial al formalizar la noción
de vecinos. Fundamentalmente, los pesos espaciales están diseñados para
**excluir algunas interacciones** al limitar el número de vecinos de una
observación, por ejemplo, considerando aquellas ubicaciones con las que
comparte una frontera.

Entonces, el **grado de interacción** entre pares de observaciones queda
definido por un efecto combinado entre el **coeficiente de correlación**
(asociado a la similitud de atributos), y su **peso** (asociado a la similitud
de ubicación).

Una vez comprendida la definición de pesos, el siguiente paso es
construir la **matriz de pesos espaciales** basada en algún tipo de criterio,
que también es una matríz de dimensiones $n\times n$ en donde se almacena a
cada peso $w_{ij}$ y que cumple con las siguientes características:

* $w_{ij}\neq 0$ para vecinos.
* $w_{ij} = 0$ implica que $i$ y $j$ no son vecinos.
* $w_{ii} = 0$ dado que no hay similitud entre una observación consigo misma.

Esto provoca que $W$ sea una **matriz rala** ya que gran número de sus
elementos son $0$, estos anulan la interacción entre múltiples observaciones
reduciendo, consecuentemente, el número de parámetros por estimar.

#### Estandarización de pesos

Otro concepto importante en la definición de la matriz de pesos espaciales es
la **estandarización por renglón** de los pesos. Esto es que para cada renglón
de la matriz de pesos se hace el escalamiento:

$$w^*_{ij} = \frac{w_{ij}}{\sum_j w_{ij}}$$

De modo que:

$$\sum_j w^*_{ij} = 1$$

Lo anterior es importante debido a:

1. Se restringe el espacio parametral de los pesos.
2. Para todos los vecinos de una observación se sabe que la suma de los pesos
será uno.
3. Vuelve el análisis comparable de una observación a otra.
4. Permite hacer un cálculo posterior llamado **rezago espacial**, referente
al promedio de los vecinos.

Como referencia se puede considerar el siguiente ejemplo de una matriz con su
respectiva estandarización por renglón:

$$A = \begin{pmatrix}
1 & 0 & 0\\
1 & 1 & 0\\
1 & 1 & 1
\end{pmatrix} \rightarrow 
A^* = \begin{pmatrix}
1 & 0 & 0\\
1/2 & 1/2 & 0\\
1/3 & 1/3 & 1/3\\
\end{pmatrix}$$

### Criterios de contigüidad

Con lo anterior resta definir las reglas para construir la matriz de pesos
espaciales, para lo cual existen múltiples **criterios de contigüidad** que
se clasifican en dos categorías principales.

#### Criterios binarios

Se trata del tipo de criterio más simple, se asignan **pesos booleanos** a las
observaciones que comparten algún tipo de frontera física. El hecho de requerir
de una frontera implica que este tipo de criterio sea utilizado en
**polígonos** de modo que:

* $w_{ij}=0$: No se comparte frontera física
* $w_{ij}=1$: Se comparte alguna frontera

La siguiente figura ilustra la idea de un criterio binario de contigüidad:

![Ejemplo de una matriz de pesos basada en criterios de contigüidad binarios](./figuras/01/contiguidad_ejemplo.png){width=350}

Sin embargo, al construir la matriz de pesos utilizando criterios binarios se
tienen complicaciones que resultan en dos enfoques a la hora de definir el qué
es una frontera:

* Matriz de **contigüidad de torres** (_Rook_): Para la definición de
contigüidad considera únicamente **fronteras completas**. Su nombre es alusivo
al movimiento de una torre en ajedrez.

* Matriz de **contigüidad de reinas** (_Queen_): Para la definición de
contigüidad considera tanto **fronteras completas** como **esquinas**. Su
nombre es alusivo al movimiento de una reina en ajedrez.

De nueva cuenta, la siguiente figura ilustra la idea detras de ambos enfoques:

![Criterios de contigüidad](./figuras/01/criterios_contiguidad.png){width=350}

Considerando el elemento central en ambos casos, bajo el criterio de torres
los vecinos serían los elementos superior, inferior, izquierdo, y derecho;
mientras que bajo el criterio de reinas todos los elementos serían vecinos.

#### Criterios basados en distancia

En general están basados en cualquier tipo de función de distancia lo cual
implica que su aplicación sea a **puntos**, por ejemplo, el centroide de
polígonos. Sin embargo, dado que una de las implicaciones de la
_1° Ley de Tobler_ es el **decaemiento por distancia** en la correlación de las
cosas, entonces no se podría utilizar directamente la distancia sino su
recíproco.

Algunos de los criterios de distancia más comunes en la práctica son:

* **Radio de cobertura**: Se debe definir un _buffer_ de cobertura basada en
una distancia de modo que aquellas observaciones dentro de cobertura son
vecinas y aquellas fuera de cobertura no:
    * $w_{ij} = 0$ si $d_{ij} > r$
    * $w_{ij} \neq 0$ si $d_{ij} \leq r$

    Sin embargo, existe un riesgo en la definición del radio de cobertura para
aquellos casos en que las regiones geográficas tienen tamaños muy diferentes
entre sí, y es que regiones pequeñas tengan demasiados vecinos, mientras que
regiones grandes tengan pocos vecinos.

    En el siguiente ejemplo, la región resaltada corre el riesgo de no tener
ningún vecino ya que la misma región puede ser más grande que el propio radio
del _buffer_.

![Ejemplo de regiones con tamaños diferentes entre sí](./figuras/01/mapa_1.png){width=250}

* **Vecinos más cercanos** (kNN): Surge como una solución al problema del
criterio anterior, y considera a los $k$ vecinos más cercanos a una observación
de interés independientemente de la distancia entre ellos, lo cual resulta en el
**mismo número de vecinos** para todas las observaciones.

### Histograma de conectividad

Una vez que se ha construido la matriz de pesos, se puede inspeccionar el
**histograma de conectividad** que no es más que un histograma del número de
vecinos por observación.

Alguno aspectos que se deben atender con detenimiento en un histograma de
conectividad son:

* Observaciones aisladas o sin vecinos, indican la presencia de islas ya sea
porque físicamente existe una isla o porque no hay datos entorno al registro
en cuestión.
* Número de vecinos inusualmente altos refleja una matriz de pesos que no
está desempeñándose correctamente, pues no consigue imponer suficiente
estructura a los datos para anular interacciones.
* Bimodalidad en la distribución.

## Autocorrelación espacial global

Como se ha venido mencionando, el primer paso para determinar autocorrelación
espacial es obteniendo un **estadístico global** que resuma tanto similitud de
atributos como similitud de ubicación, siendo **Moran's I** el más popular.

### Estadístico Moran's I

En la sección [Estadísticos para autocorrelación espacial](#estadísticos-para-autocorrelación-espacial)
se mencionó que la construcción de un estadístico debe ser de la forma:

$$z=\sum_{ij}f(x_i,x_j)\cdot w_{ij}$$

Para este fin, la formulación más común de Moran's I es la siguiente:

$$(1):\ I = \frac{N}{S_0}\frac{\sum_i^N\sum_j^N w_{ij}(x_i-\bar x)(x_j-\bar x)}{\sum_i^N (x_i-\bar x)^2}$$

En donde los términos $(x_i-\bar x)$, $(x_j-\bar x)$ son la desviación de una
observación con respecto a la media de modo que en el comportamiento general
queden centradas en $0$, y en lo subsecuente se reemplazarán por $z_i$, $z_j$
respectivamente. Adicionalmente:

* $N$: Número de observaciones
* $S_0$: Suma de todos los pesos espaciales $\sum_{ij}w_{ij}$

Para efectos de interpretación se puede reformular de la siguiente manera:

$$(2):\ I = \left(\frac{\sum_{ij}w_{ij}(z_i\cdot z_j)}{S_0}\right) \left(\frac{1}{N}\sum_i z_i^2\right)^{-1}$$

Con lo cual se debe destacar los siguiente:

* El término $\frac{1}{N}\sum_i z_i^2$ es una estimación de la
[varianza](https://es.wikipedia.org/wiki/Varianza).
* Si los pesos fueron estandarizados por fila entonces la suma
$S_0=\sum_{ij} w_{ij}^*=N$, con lo cual se puede ver que el término $\frac{1}{S_0}(z_i\cdot z_j)$
es análogo a la [correlación de Pearson](https://es.wikipedia.org/wiki/Coeficiente_de_correlaci%C3%B3n_de_Pearson).
Esta similitud es una de las razones por las que Moran's I es el estadístico
más común.

Así mismo, si los pesos fueron estandarizados por fila entonces el estadístico
se simplifica a la formulación:

$$(3):\ I = \frac{\sum_{ij}w_{ij}(z_i\cdot z_j)}{\sum_{i} z_i^2}$$

::: {.callout-important}
La prueba de autocorrelación espacial global es potente pero presenta
algunas limitantes ya que se trata de una prueba que determina si hay evidencia
de estructura espacial o _clusters_ en los datos. Sin embargo, por sí misma
**no identifica los _clusters_ ni en donde se encuentran ubicados**.
:::

### Rezago espacial

Con la formulación $(3)$ de _Moran's I_, que considera que los pesos fueron
estandarizados previamente, se puede visualizar de forma más directa una nueva
forma de representar el estadístico:

$$(4):\ I = \frac{\sum_iz_i \left(\sum_j w_{ij}\cdot z_j \right)}{\sum_i z_i^2}$$

Bajo esta nueva formulación, el término $\sum_j w_{ij}\cdot z_j$ se conoce como
el **rezago espacial** de la observación $i$, que es un promedio ponderado de
la variable de interés $z$ en los vecinos de la observación $i$. Haciendo un
cambio de variable $l_i = \sum_j w_{ij}\cdot z_j$, entonces:

$$I = \frac{\sum_iz_i\cdot l_i}{\sum_i z_i^2}$$

Lo cual hace evidente que _Moran's I_ es un
[coeficiente de regresión ](https://en.wikipedia.org/wiki/Simple_linear_regression)
para la línea de ajuste entre la variable de interés $z$ y su rezago espacial
$l$, por tanto, el estadístico $I$ es la pendiente de la línea de ajuste.

### Interpretación

Establecido el significado el estadístico Moran's I, entonces su interpretación
depende paralelamente tanto de la **señal** como de la **significancia**. Lo
primero que se debe asegurar antes de empezar hacer cualquier interpretación es
que sea **estadísticamente significativo**, de otro modo, la señal por sí misma
no dice absolutamente nada. Este proceso se describe con mayor detenimiento en
la sección [Significancia estadística](#significancia-estadística).

Dicho eso, una vez asegurada la significancia, entonces:

* Si la señal es positiva, entonces hay evidencia de **_clusters_**, tomando en
cuenta que estos son referentes a **valores similares en una vecindad** y no
necesariamente tienen que ser positivos o negativos.
* Si la señal es negativa, entonces hay evidencia de **valores alternantes** o
autocorrelación espacial negativa.

Para ahondar aún más en su interpretación, se puede considerar la siguiente
dispersión de Moran para la cual se determinó el estadístico $I$:

![Ejemplo de dispersión de Moran](./figuras/01/dispersion_moran.png){height=300}

Se debe destacar el significado de los **cuatro cuadrantes** que aparecen en la
dispersión de Moran:

* Los cuadrantes **superior derecho** e **inferior izquierdo** corresponden a
autocorrelación espacial **positiva**, lo cual significa que estos datos son
**similares a sus vecinos**.
* Los cuadrantes **superior izquierdo** e **inferior derecho** corresponden a
autocorrelación espacial **negativa**, lo cual significa que en estos datos hay
**valores alternantes** con sus vecinos lo cual puede ser indicativo de
**_outliers_** en la región.

Tomando en cuenta que esta dispersión se obtiene con la distancia en
desviaciones estándar de la media de los datos, entonces se puede inferir lo
siguiente:

* Los puntos en el cuadrante **superior derecho** son _clusters_ por encima de
la media, por lo que corresponden a **Hot Spots**.
* Los puntos en el cuadrante **inferior izquierdo** son _clusters_ por debajo
de la media, por lo que corresponden a **Cold Spots**.
* Los puntos en el cuadrante **superior izquierdo** son observaciones con
**alternancia negativa** con respecto a sus vecinos, es decir, son más chicos
que sus vecinos.
* Los puntos en el cuadrante **inferior derecho** son observaciones con
**alternancia positiva** con respecto a sus vecinos, es decir, son más grandes
que sus vecinos.

### Significancia estadística

Como se mencionó en la sección anterior, la señal del estadístico _Moran's I_
no es indicativa de nada si no es estadísticamente significativa. Para
determinar esto, se utiliza el **valor-p** asociado a la prueba de hipótesis.
Recordando que las hipótesis son:

* $H_0$: Los datos se distribuyen bajo aleatoriedad espacial.
* $H_1$: Los datos presentan estructura espacial.

Entonces, un mecanismo que puede utilizarse para probar esto es mediante
**permutaciones** de los datos. Dado que la $H_0$ plantea que los datos se
generan bajo aleatoriedad espacial, entonces las permutaciones 
**reasignan aleatoriamente la variable** de estudio a las distintas
observaciones **un número grande de veces**, y para cada permutación se calcula
el estadístico _Moran's I_. Una vez que se han realizado todas las
permutaciones, se genera la **distribución de muestreo** para el estadístico y
se determina el **valor-p**.

El valor-p indica la probabilidad de observar los datos bajo la distribución de
muestreo del estimador, por lo que si la probabilidad es muy baja entonces muy
posiblemente exista estructura espacial en los datos, con lo cual el
estadístico _Moran's I_ es **estadísticamente significativo**.

De nueva cuenta se puede considerar la siguiente dispersión de Moran junto con
su prueba de significancia:

![Prueba de significancia de una dispersión de Moran](./figuras/01/significancia_moran.png){height=300}

En este caso el estadístico de los datos está extremadamente alejado de la
distribución muestral lo cual resulta en un $p_{val}=0.1\%$ con lo que se
asegura que es estadísticamente significativo.

## Autocorrelación espacial local

Hasta este punto se ha visto que el estadístico Moran's I es ampliamente
utilizado para identificar si existe estructura espacial en los datos así como
su tipo, sea positiva o negativa. Sin embargo, este no indica en donde pudieran
estar ubicados los segmentos geográficos en los cuales se están agrupando los
datos.

El objetivo de la autocorrelación espacial local es encontrar aquellos grupos
en los que los datos son más similares (o menos similares) comparados con la
aleatoriedad espacial, sus ubicaciones en específico, así como una medida de la
significancia estadística de estos.

Lo anterior se consigue utilizando indicadores conocidos como **LISA**,
indicadores locales de asociación espacial por sus siglas en inglés, que tienen
dos características importantes:

1. Asignan una medida de significancia estadística a nivel local para cada uno
de los elementos de análisis.
2. Establecen una relación proporcional entre la suma de los estadísticos
locales con su correspondiente estadístico global.

En términos simples, un estadístico de autocorrelación espacial global se puede
expresar como una doble sumatoria sobre las ubicaciones $i,j$. Dado esto, la
forma local de dicho estadístico sería la suma sobre las diferentes ubicaciones
$j$ del estadístico en cuestión, para cada una de las ubicaciones $i$, es decir,
si el estadístico global es de la forma:

$$\sum_{i}\sum_{j}f(x_i,x_j)\cdot w_{ij}$$

Una expresión generalizada para la forma de los indicadores locales sería:

$$\sum_{j}f(x_i,x_j)\cdot w_{ij}$$

### Moran Local

El estadístico de **Moran local** es por mucho el más utilizado y, como sugiere
su nombre, se deriva del estadístico global _Moran's I_.

Recordandor la formulación $(3)$ de Moran, resultado de haber estandarizado por
renglón:

$$I = \frac{\sum_{ij}w_{ij}(z_i\cdot z_j)}{\sum_{i} z_i^2}$$

Utilizando la lógica planteada para los indicadores locales se tendría un
estadístico para cada una de las ubicaciones $i$ de la forma:

$$I_{i} = \frac{\sum_{j}w_{ij}(z_i\cdot z_j)}{\sum_{i} z_i^2}$$

En la expresión anterior, el denomindador es constante y se puede sustituir por
$a$ para efectos prácticos. A su vez, el término $z_i$ en el numerador puede
factorizarse de la ecuación, resultando en:

$$I_{i} = a\cdot z_i\sum_{j}w_{ij}\cdot z_j$$

Que es la expresión de Moran local, y con la cual es posible identificar que
estos indicadores son el producto del valor en la ubicación $i$ con su rezago
espacial, incorporando un factor de escalamiento de $a$.